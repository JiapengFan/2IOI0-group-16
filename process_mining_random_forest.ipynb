{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from training.predictionAlgo import naiveNextEventPredictor\n",
    "from preprocessing.dataParsing import parseData\n",
    "from preprocessing.dataSplitting import dataSplitter\n",
    "\n",
    "# Convert csv into dataframe\n",
    "df_training_raw = pd.read_csv('.\\data\\BPI2012Training.csv')\n",
    "df_test_raw = pd.read_csv('.\\data\\BPI2012Test.csv')\n",
    "\n",
    "# Parsing data\n",
    "(df_training, df_2012_last_event_per_case_train) = parseData(df_training_raw)\n",
    "(df_test, df_2012_last_event_per_case_test) = parseData(df_test_raw)\n",
    "\n",
    "# Clean and split the data into train, validation & test data\n",
    "(df_training, df_validation, df_test) = dataSplitter(df_training, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "unique_training_events = df_training['event concept:name'].unique().reshape(-1, 1)\n",
    "\n",
    "# Define One-hot encoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "onehot_encoder = onehot_encoder.fit(unique_training_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "number_events_mean = df_training.groupby('case concept:name').count()['event concept:name'].mean()\n",
    "number_events_mean = ceil(number_events_mean)\n",
    "number_events_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.000000\n",
      "1     0.000000\n",
      "2     0.000000\n",
      "3     0.000000\n",
      "4     0.000000\n",
      "        ...   \n",
      "95    0.002495\n",
      "96    0.002495\n",
      "97    0.002495\n",
      "98    0.002495\n",
      "99    0.002495\n",
      "Name: unix_reg_time, Length: 100, dtype: float64\n",
      "0         1.317422e+09\n",
      "1         1.317422e+09\n",
      "2         1.317422e+09\n",
      "3         1.317422e+09\n",
      "4         1.317422e+09\n",
      "              ...     \n",
      "193442    1.328285e+09\n",
      "193443    1.328285e+09\n",
      "193444    1.328285e+09\n",
      "193445    1.328285e+09\n",
      "193446    1.328285e+09\n",
      "Name: unix_reg_time, Length: 193447, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df_relevant = df_training[['case concept:name', 'event concept:name', 'actual_next_event', 'case AMOUNT_REQ', 'unix_reg_time']].copy()\n",
    "\n",
    "# Normalise time in seconds from case registeration to current event\n",
    "time_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "reg_time = df_relevant['unix_reg_time'].to_numpy().reshape(-1, 1)\n",
    "df_relevant['unix_reg_time'] = np.around(loan_scaler.fit_transform(reg_time), decimals = 4)\n",
    "print(df_relevant['unix_reg_time'])\n",
    "print(df_training['unix_reg_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def createInputRF(df):\n",
    "    # df with only relevant training data, i.e. loan amount, current event, next event and time elapsed since registeration.\n",
    "    df_relevant = df[['case concept:name', 'event concept:name', 'actual_next_event', 'case AMOUNT_REQ', 'unix_reg_time']].copy()\n",
    "\n",
    "    # One-hot encode current and next event\n",
    "    training_current_event = df_relevant['event concept:name'].to_numpy().reshape(-1, 1)\n",
    "    df_relevant['event concept:name'] = onehot_encoder.transform(training_current_event).tolist()\n",
    "\n",
    "    training_next_event = df_relevant['actual_next_event'].to_numpy().reshape(-1, 1)\n",
    "    df_relevant['actual_next_event'] = onehot_encoder.transform(training_next_event).tolist()\n",
    "\n",
    "    # Normalise loan amount\n",
    "    loan_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    case_amount = df_relevant['case AMOUNT_REQ'].to_numpy().reshape(-1, 1)\n",
    "    df_relevant['case AMOUNT_REQ'] = np.around(loan_scaler.fit_transform(case_amount), decimals = 4)\n",
    "\n",
    "    # Normalise time in seconds from case registeration to current event\n",
    "    time_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    reg_time = df_relevant['unix_reg_time'].to_numpy().reshape(-1, 1)\n",
    "    df_relevant['unix_reg_time'] = np.around(loan_scaler.fit_transform(reg_time), decimals = 4)\n",
    "    print(df_relevant['unix_reg_time'])\n",
    "\n",
    "    # Prepare input and output in form of [samples, features]\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    # Get groupby object df by case id\n",
    "    df_groupby_case_id = df_relevant.groupby('case concept:name')\n",
    "\n",
    "    # Unique case ids\n",
    "    unique_case_ids = df_relevant['case concept:name'].unique().tolist()\n",
    "\n",
    "    # Find input and output vector in form of [samples, features]\n",
    "    for unique_id in unique_case_ids:\n",
    "        xy_unique_id = df_groupby_case_id.get_group(unique_id)[['event concept:name', 'actual_next_event', 'case AMOUNT_REQ', 'unix_reg_time']].values.tolist()\n",
    "\n",
    "        base_case = xy_unique_id[0][0:2].copy()\n",
    "        x_first_sample_per_case = base_case[0].copy()\n",
    "        x_first_sample_per_case.extend([xy_unique_id[0][2], xy_unique_id[0][3]])\n",
    "        x.append(x_first_sample_per_case)\n",
    "        y.append(base_case[1].copy())\n",
    "\n",
    "        # event[0] = current event, event[1] = next event, event[2] = loan amount, event[3] = time elapsed since registeration of case\n",
    "        for event in xy_unique_id[1:]:\n",
    "            base_case[0] = [prev_xs + current_x for prev_xs, current_x in zip(base_case[0], event[0])]\n",
    "            x_sample = base_case[0].copy()\n",
    "            x_sample.extend([event[2], event[3]])\n",
    "            x.append(x_sample)\n",
    "            y.append(event[1])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0.0\n",
      "1         0.0\n",
      "2         0.0\n",
      "3         0.0\n",
      "4         0.0\n",
      "         ... \n",
      "193442    1.0\n",
      "193443    1.0\n",
      "193444    1.0\n",
      "193445    1.0\n",
      "193446    1.0\n",
      "Name: unix_reg_time, Length: 193447, dtype: float64\n",
      "0        0.0000\n",
      "1        0.0000\n",
      "2        0.0000\n",
      "3        0.0002\n",
      "4        0.0002\n",
      "          ...  \n",
      "38039    1.0000\n",
      "38040    1.0000\n",
      "38096    1.0000\n",
      "38099    1.0000\n",
      "38100    1.0000\n",
      "Name: unix_reg_time, Length: 47823, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Determine actual next event\n",
    "(df_training, df_validation) = naiveNextEventPredictor(df_training, df_validation)\n",
    "(df_test, df_validation) = naiveNextEventPredictor(df_test, df_validation)\n",
    "\n",
    "x_train, y_train = createInputRF(df_training)\n",
    "x_test, y_test = createInputRF(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a36cfa25ef9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Convert y_train to format [samples, features]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Convert y_train to format [samples, features]\n",
    "y_train = np.reshape(y_train, (-1, len(y_train[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding random forest parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [64, 136, 209, 282, 355, 428], 'max_features': ['sqrt'], 'max_depth': [5, 28, 52, 76, 100], 'min_samples_split': [0.02, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3], 'min_samples_leaf': [0.02, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3], 'bootstrap': [True]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 64, stop = 428, num = 6)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 100, num = 5)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [float(round(x)) * 0.01 for x in np.linspace(5, 30, num = 6)]\n",
    "min_samples_split.insert(0, 0.02)\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [float(round(x)) * 0.01 for x in np.linspace(5, 30, num = 6)]\n",
    "min_samples_leaf.insert(0, 0.02)\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 25, cv = 3, verbose=1, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_para_dict = rf_random.best_params_\n",
    "best_para_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./BestParaRF/para{}.json'.format(15), 'w') as fp:\n",
    "    json.dump(best_para_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 184, 'min_samples_split': 0.25, 'min_samples_leaf': 0.02, 'max_features': 'sqrt', 'max_depth': 100, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "with open('./BestParaRF/para10.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 264, 'min_samples_split': 0.05, 'min_samples_leaf': 0.02, 'max_features': 'sqrt', 'max_depth': 76, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "with open('./BestParaRF/para11.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 104, 'min_samples_split': 0.02, 'min_samples_leaf': 0.05, 'max_features': 'sqrt', 'max_depth': 5, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "with open('./BestParaRF/para12.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 224, 'min_samples_split': 0.02, 'min_samples_leaf': 0.02, 'max_features': 'sqrt', 'max_depth': 76, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "with open('./BestParaRF/para13.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 264, 'min_samples_split': 0.15, 'min_samples_leaf': 0.02, 'max_features': 'sqrt', 'max_depth': 52, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "with open('./BestParaRF/para14.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [428, 264], 'max_features': ['sqrt'], 'max_depth': [200, 400], 'min_samples_split': [0.02, 0.05], 'min_samples_leaf': [0.02, 0.05], 'bootstrap': [True]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [428, 264]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [200, 400]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [0.02, 0.05]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [0.02, 0.05]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True]\n",
    "# Create the random grid\n",
    "grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = GridSearchCV(estimator = rf, param_grid = grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=400, max_features='sqrt',\n",
       "                       min_samples_leaf=0.02, min_samples_split=0.02,\n",
       "                       n_estimators=800)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 800, min_samples_split = 0.02, min_samples_leaf = 0.02, max_depth = 400, max_features = 'sqrt', bootstrap = True)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5140413608514731"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
