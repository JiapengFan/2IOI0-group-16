{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from training.predictionAlgo import naiveNextEventPredictor\n",
    "from preprocessing.dataParsing import parseData\n",
    "from preprocessing.dataSplitting import dataSplitter\n",
    "\n",
    "# Convert csv into dataframe\n",
    "df_training_raw = pd.read_csv('.\\data\\BPI2012Training.csv')\n",
    "df_test_raw = pd.read_csv('.\\data\\BPI2012Test.csv')\n",
    "\n",
    "# Parsing data\n",
    "(df_training, df_2012_last_event_per_case_train) = parseData(df_training_raw)\n",
    "(df_test, df_2012_last_event_per_case_test) = parseData(df_test_raw)\n",
    "\n",
    "# Clean and split the data into train, validation & test data\n",
    "(df_training, df_validation, df_test) = dataSplitter(df_training, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "unique_training_events = df_training['event concept:name'].unique().reshape(-1, 1)\n",
    "\n",
    "# Define One-hot encoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "onehot_encoder = onehot_encoder.fit(unique_training_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "number_events_mean = df_training.groupby('case concept:name').count()['event concept:name'].mean()\n",
    "number_events_mean = ceil(number_events_mean)\n",
    "number_events_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Determine actual next event\n",
    "(df_training, df_validation) = naiveNextEventPredictor(df_training, df_validation)\n",
    "\n",
    "# df with only relevant training data, i.e. loan amount, current event, next event and time elapsed since registeration.\n",
    "df_training_relevant = df_training[['case concept:name', 'event concept:name', 'actual_next_event', 'case AMOUNT_REQ', 'unix_reg_time']].copy()\n",
    "\n",
    "# One-hot encode current and next event\n",
    "training_current_event = df_training_relevant['event concept:name'].to_numpy().reshape(-1, 1)\n",
    "df_training_relevant['event concept:name'] = onehot_encoder.transform(training_current_event).tolist()\n",
    "\n",
    "training_next_event = df_training_relevant['actual_next_event'].to_numpy().reshape(-1, 1)\n",
    "df_training_relevant['actual_next_event'] = onehot_encoder.transform(training_next_event).tolist()\n",
    "\n",
    "# Normalise loan amount\n",
    "loan_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "case_amount = df_training_relevant['case AMOUNT_REQ'].to_numpy().reshape(-1, 1)\n",
    "df_training_relevant['case AMOUNT_REQ'] = np.around(loan_scaler.fit_transform(case_amount), decimals = 3)\n",
    "\n",
    "# Normalise time in seconds from case registeration to current event\n",
    "time_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "reg_time = df_training_relevant['unix_reg_time'].to_numpy().reshape(-1, 1)\n",
    "df_training_relevant['unix_reg_time'] = np.around(loan_scaler.fit_transform(reg_time), decimals = 3)\n",
    "\n",
    "# Prepare input and output in form of [samples, features]\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "# Get groupby object df by case id\n",
    "df_training_groupby_case_id = df_training_relevant.groupby('case concept:name')\n",
    "\n",
    "# Unique case ids\n",
    "unique_case_ids = df_training_relevant['case concept:name'].unique().tolist()\n",
    "\n",
    "# Find input and output vector in form of [samples, features]\n",
    "for unique_id in unique_case_ids:\n",
    "    xy_train_unique_id = df_training_groupby_case_id.get_group(unique_id)[['event concept:name', 'actual_next_event', 'case AMOUNT_REQ', 'unix_reg_time']].values.tolist()\n",
    "    \n",
    "    base_case = xy_train_unique_id[0][0:2].copy()\n",
    "    x_train_first_sample_per_case = base_case[0].copy()\n",
    "    x_train_first_sample_per_case.extend([xy_train_unique_id[0][2], xy_train_unique_id[0][3]])\n",
    "    x_train.append(x_train_first_sample_per_case)\n",
    "    y_train.append(base_case[1].copy())\n",
    "\n",
    "    # event[0] = current event, event[1] = next event, event[2] = loan amount, event[3] = time elapsed since registeration of case\n",
    "    for event in xy_train_unique_id[1:]:\n",
    "        base_case[0] = [prev_xs + current_x for prev_xs, current_x in zip(base_case[0], event[0])]\n",
    "        x_train_sample = base_case[0].copy()\n",
    "        x_train_sample.extend([event[2], event[3]])\n",
    "        x_train.append(x_train_sample)\n",
    "        y_train.append(event[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193447, 24)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Convert y_train to format [samples, features]\n",
    "y_train = np.reshape(y_train, (-1, len(y_train[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding random forest parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [64, 104, 144, 184, 224, 264], 'max_features': ['sqrt'], 'max_depth': [5, 28, 52, 76, 100], 'min_samples_split': [0.02, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3], 'min_samples_leaf': [0.02, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3], 'bootstrap': [True]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 64, stop = 264, num = 6)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 100, num = 5)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [float(round(x)) * 0.01 for x in np.linspace(5, 30, num = 6)]\n",
    "min_samples_split.insert(0, 0.02)\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [float(round(x)) * 0.01 for x in np.linspace(5, 30, num = 6)]\n",
    "min_samples_leaf.insert(0, 0.02)\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(random_state=42),\n",
       "                   n_iter=1, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True],\n",
       "                                        'max_depth': [5, 28, 52, 76, 100],\n",
       "                                        'max_features': ['sqrt'],\n",
       "                                        'min_samples_leaf': [0.02, 0.05, 0.1,\n",
       "                                                             0.15, 0.2, 0.25,\n",
       "                                                             0.3],\n",
       "                                        'min_samples_split': [0.02, 0.05, 0.1,\n",
       "                                                              0.15, 0.2, 0.25,\n",
       "                                                              0.3],\n",
       "                                        'n_estimators': [64, 104, 144, 184, 224,\n",
       "                                                         264]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 1, cv = 3, verbose=2, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 64,\n",
       " 'min_samples_split': 0.05,\n",
       " 'min_samples_leaf': 0.15,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 76,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_para_dict = rf_random.best_params_\n",
    "best_para_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./BestParaRF/para{}.json'.format(1), 'w') as fp:\n",
    "    json.dump(best_para_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
